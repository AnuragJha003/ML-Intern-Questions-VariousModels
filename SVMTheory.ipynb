{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c42b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "How To Learn Machine Learning Algorithms For Interviews\n",
    "SVM\n",
    "Theoretical Understanding:\n",
    "\n",
    "https://www.youtube.com/watch?v=H9yACitf-KM\n",
    "https://www.youtube.com/watch?v=Js3GLb1xPhc\n",
    "1. What Are the Basic Assumption?\n",
    "There are no such assumptions\n",
    "\n",
    "2. Advantages\n",
    "SVM is more effective in high dimensional spaces.\n",
    "SVM is relatively memory efficient.\n",
    "SVMâ€™s are very good when we have no idea on the data.\n",
    "Works well with even unstructured and semi structured data like text, Images and trees.\n",
    "The kernel trick is real strength of SVM. With an appropriate kernel function, we can solve any complex problem.\n",
    "SVM models have generalization in practice, the risk of over-fitting is less in SVM.\n",
    "3. Disadvantages\n",
    "More Training Time is required for larger dataset\n",
    "It is difficult to choose a good kernel function https://www.youtube.com/watch?v=mTyT-oHoivA\n",
    "The SVM hyper parameters are Cost -C and gamma. It is not that easy to fine-tune these hyper-parameters. It is hard to visualize their impact\n",
    "4. Whether Feature Scaling is required?\n",
    "Yes\n",
    "\n",
    "5. Impact of Missing Values?\n",
    "Although SVMs are an attractive option when constructing a classifier, SVMs do not easily accommodate missing covariate information. Similar to other prediction and classification methods, in-attention to missing data when constructing an SVM can impact the accuracy and utility of the resulting classifier.\n",
    "\n",
    "6. Impact of outliers?\n",
    "It is usually sensitive to outliers https://arxiv.org/abs/1409.0934#:~:text=Despite%20its%20popularity%2C%20SVM%20has,causes%20the%20sensitivity%20to%20outliers.\n",
    "\n",
    "Types of Problems it can solve(Supervised)\n",
    "Classification\n",
    "Regression\n",
    "Overfitting And Underfitting\n",
    "In SVM, to avoid overfitting, we choose a Soft Margin, instead of a Hard one i.e. we let some data points enter our margin intentionally (but we still penalize it) so that our classifier don't overfit on our training sample\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "Different Problem statement you can solve using Naive Baye's\n",
    "We can use SVM with every ANN usecases\n",
    "Intrusion Detection\n",
    "Handwriting Recognition\n",
    "Practical Implementation\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "Performance Metrics\n",
    "Classification\n",
    "Confusion Matrix\n",
    "Precision,Recall, F1 score\n",
    "Regression\n",
    "R2,Adjusted R2\n",
    "MSE,RMSE,MAE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
